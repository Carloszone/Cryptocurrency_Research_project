{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crypto_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzZm7Z82LW0kKFGUPHZYrA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carloszone/Crypto_Token_Research/blob/main/Crypto_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NDkqHsqctPs"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNmS_rbucsC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0070d77a-c573-4a6d-9ba5-16c4412d64ad"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import re\n",
        "from sklearn import preprocessing\n",
        "import os\n",
        "import time\n",
        "from google.colab import files\n",
        "from datetime import date\n",
        "from datetime import datetime\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ELRqI1-PXAOL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9Te382kc3_G"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbdComJqpM8N"
      },
      "source": [
        "# module 1: LTE\n",
        "# function to load data from df path\n",
        "def load_df(token_path, transaction_path, index_path, commodity_path):\n",
        "  token_df = pd.read_csv(token_path, parse_dates = ['Date'])\n",
        "  transaction_df = pd.read_csv(transaction_path, parse_dates = ['Date'])\n",
        "  index_df = pd.read_csv(index_path, parse_dates = ['Date'])\n",
        "  commodity_df = pd.read_csv(commodity_path, parse_dates = ['Date'])\n",
        "  return token_df, transaction_df, index_df, commodity_df\n",
        "\n",
        "\n",
        "# function to convert unit from wei to ether\n",
        "def wei_2_ether(num):\n",
        "  res = num/10**18\n",
        "  return res\n",
        "\n",
        "# function to transfrom transaction_df\n",
        "def transform_transaction_df(df):\n",
        "  df['total_value_per_day'] =  df['total_value_per_day'].astype('float64')\n",
        "\n",
        "  trans_df = df.copy()\n",
        "  trans_df['total_value_per_day'] = df['total_value_per_day'].apply(lambda x: wei_2_ether(x))\n",
        "  trans_df['avg_value_per_transaction'] = df['avg_value_per_transaction'].apply(lambda x: wei_2_ether(x)) #\n",
        "  trans_df['total_gas_per_day'] = df['total_gas_per_day'].apply(lambda x: wei_2_ether(x)) #\n",
        "  trans_df['total_gas_used_per_day'] = df['total_gas_used_per_day'].apply(lambda x: wei_2_ether(x)) #\n",
        "  trans_df['avg_gas_used_per_transaction'] = df['avg_gas_used_per_transaction'].apply(lambda x: wei_2_ether(x)) #\n",
        "  trans_df['total_transaction_fee'] = df['total_transaction_fee'].apply(lambda x: wei_2_ether(x))\n",
        "  trans_df['total_priority_fee'] = df['total_priority_fee'].apply(lambda x: wei_2_ether(x))\n",
        "  trans_df['avg_transaction_fee_per_transaction'] = df['avg_transaction_fee_per_transaction'].apply(lambda x: wei_2_ether(x)) #\n",
        "  trans_df['avg_priority_fee_per_transaction'] = df['avg_priority_fee_per_transaction'].apply(lambda x: wei_2_ether(x)) #\n",
        "  return trans_df\n",
        "\n",
        "# function to fill na values with the previous non-na values\n",
        "def fill_na(merged_df):\n",
        "  df = merged_df.copy()\n",
        "  # na check\n",
        "  missing = df.isna().sum().sort_values(ascending = False)\n",
        "  percent_missing = ((missing / df.isnull().count()) * 100).sort_values(ascending = False)\n",
        "  missing_df = pd.concat([missing,percent_missing], axis = 1, keys = ['Total', 'Percent'],sort = False)\n",
        "\n",
        "  # fill na\n",
        "  columns = set(missing_df[missing_df['Total'] >= 1].reset_index()['index'])\n",
        "  \n",
        "  for col in columns:\n",
        "    null_index = df.index[df[col].isnull() == True].tolist()\n",
        "    for ind in null_index:\n",
        "      df.loc[ind, col] = df.loc[ind-1, col]\n",
        "  return df, missing_df\n",
        "\n",
        "# function to merged dataframes\n",
        "def merge_df(token_df, transaction_df, index_df, commodity_df):\n",
        "\n",
        "  merged_df = token_df.merge(transaction_df, how = 'left', on = 'Date', suffixes = ('_token', '_transaction'))\n",
        "\n",
        "  indexes = set(index_df['name'])\n",
        "  commodities = set(commodity_df['name'])\n",
        "  for index in indexes:\n",
        "    merged_df = merged_df.merge(index_df[index_df['name'] == index], how = 'left', on = 'Date', suffixes = (None, f'_{index}'))\n",
        "  for commodity in commodities:\n",
        "    merged_df = merged_df.merge(commodity_df[commodity_df['name'] == commodity], how = 'left', on = 'Date', suffixes = (None, f'_{commodity}'))\n",
        "   \n",
        "  return merged_df\n",
        "\n",
        "# function to extract needed data with regular expression\n",
        "def extract_data(df, regex, fixed_list):\n",
        "  columns = list(df.columns)\n",
        "  target = re.compile(regex)\n",
        "  target_list = list(filter(target.match, columns))\n",
        "\n",
        "  if 'Date' in target_list or 'Date' in fixed_list:\n",
        "    lists = target_list + fixed_list\n",
        "  else:\n",
        "    lists = ['Date'] + target_list + fixed_list\n",
        "  res = df.loc[:,lists].set_index('Date')\n",
        "  return res\n",
        "\n",
        "# function to identify weekends\n",
        "def weekend_check(_datetime):\n",
        "  if date.weekday(_datetime) >=5:\n",
        "    res = 1\n",
        "  else:\n",
        "    res = 0\n",
        "  return res\n",
        "\n",
        "# function to generate target df for the following process(visualizing and modeling)\n",
        "def generate_target_df(token_path, transaction_path, index_path, commodity_path, fixed_list, regex, token_name):\n",
        "  token_df, transaction_df, index_df, commodity_df = load_df(token_path, transaction_path, index_path, commodity_path)\n",
        "  token_df = token_df[token_df.name == token_name]\n",
        "  transaction_df = transform_transaction_df(transaction_df)\n",
        "  merged_df = merge_df(token_df, transaction_df, index_df, commodity_df)\n",
        "  merged_df, missing_df  = fill_na(merged_df)\n",
        "  merged_df['weekend'] = merged_df['Date'].apply(weekend_check)\n",
        "  fixed_list = fixed_list + list(transaction_df.columns)\n",
        "  target_df = extract_data(merged_df, regex, fixed_list)\n",
        "  return target_df, missing_df\n",
        "\n",
        "# function to get df shift\n",
        "def df_shift(dataset, y_label, shift_n = 1):\n",
        "  df = dataset.copy()\n",
        "  new_col = y_label + '_p'\n",
        "  df[new_col] = df[y_label].shift(periods= shift_n)\n",
        "  df = df.dropna()\n",
        "  return df\n",
        "\n",
        "# function to split df into trainset and testset\n",
        "def split_df(df, timedate):\n",
        "  trainset = df[df.index <= timedate]\n",
        "  testset = df[df.index > timedate]\n",
        "  return trainset, testset\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIlUu0YVbeOg"
      },
      "source": [
        "# modelue 2: visualization\n",
        "\n",
        "# funtion to draw lineplot\n",
        "def draw_lineplot(df, cols, y, second_axis = True):\n",
        "  plot_df = df.loc[:,cols]\n",
        "\n",
        "  fig, ax1 = plt.subplots()\n",
        "  if second_axis == True:\n",
        "    ax2 = ax1.twinx()\n",
        "    sns.lineplot(x = plot_df.index, y = y[0], ax = ax1, color = 'blue', label = y[0], data = plot_df)\n",
        "    sns.lineplot(x = plot_df.index, y = y[1], ax = ax2, color = 'red', label = y[1], data = plot_df)\n",
        "  else:\n",
        "    ax1 = sns.lineplot(data = plot_df)\n",
        "  ax1.tick_params(axis='x', rotation=90)\n",
        "  ax1.legend(loc=0)\n",
        "  ax2.legend(loc=0)\n",
        "  plt.show()\n",
        "\n",
        "# funtion to output correlation matrix\n",
        "def correlation_matrix(df):\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  corrMatrix = df.corr()\n",
        "  ax = sns.heatmap(corrMatrix, annot=True)\n",
        "  plt.show()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWnILSwCYy00"
      },
      "source": [
        "# module 3: linear regression model\n",
        "\n",
        "# function to normalize df\n",
        "def df_preprocessing(df, type = 'standardize'):\n",
        "  X = df.values\n",
        "  if type == 'standardize':\n",
        "    std_scaler = preprocessing.StandardScaler().fit(X)\n",
        "    x_scaled = std_scaler.transform(X)\n",
        "    res = pd.DataFrame(x_scaled, columns=df.columns, index = df.index)\n",
        "    return res, std_scaler\n",
        "  elif type == 'minmax':\n",
        "    minmax_scaler = preprocessing.MinMaxScaler().fit(X)\n",
        "    x_scaled = minmax_scaler.transform(X)\n",
        "    res = pd.DataFrame(x_scaled, columns=df.columns, index = df.index)\n",
        "    return res, minmax_scaler\n",
        "  \n",
        "\n",
        "# function to get data for modelling\n",
        "def get_data(df, y_label, previous = False, shift_n = 1, preprocess = None, intercept = True):\n",
        "  if previous == True:\n",
        "    df = df_shift(df, y_label = y_label, shift_n = shift_n)\n",
        "  \n",
        "  # 01 split X and Y\n",
        "  X = df.loc[:, df.columns != y_label]\n",
        "  Y = df.loc[:, df.columns == y_label]\n",
        "  \n",
        "  # 02 preprocess\n",
        "  scaler = None\n",
        "  if preprocess == 'standardize':\n",
        "    X, scaler = df_preprocessing(X, type = 'standardize')\n",
        "  if preprocess == 'minmax':\n",
        "    X, scaler = df_preprocessing(X, type = 'minmax')\n",
        "\n",
        "  # 03 add constant term\n",
        "  if intercept == True:\n",
        "    X = sm.add_constant(X)\n",
        "  return X, Y, scaler\n",
        "\n",
        "def alpha_search(x, y, alpha = None, type = 'simple'):\n",
        "  if type == 'ridge':\n",
        "    ridge_cv = RidgeCV(alphas= alpha)\n",
        "    model_cv = ridge_cv.fit(x, y)\n",
        "    return model_cv.alpha_\n",
        "  if type == 'lasso':\n",
        "    lasso_cv = LassoCV(alphas= alpha)\n",
        "    model_cv = lasso_cv.fit(x, y)\n",
        "    return model_cv.alpha_\n",
        " \n",
        "def liner_model(X, Y, type = 'simple', alpha = None):\n",
        "  model = sm.OLS(Y, X)\n",
        "  results_fu = model.fit()\n",
        "  Best_alpha = None\n",
        "  if type == 'ridge':\n",
        "    best_alpha = alpha_search(X, Y, alpha = alpha, type = 'ridge')\n",
        "    model_ridge = model.fit_regularized(L1_wt=0, alpha= best_alpha, start_params=results_fu.params)\n",
        "    ridge_result = sm.regression.linear_model.OLSResults(model, model_ridge.params, model.normalized_cov_params)\n",
        "    return ridge_result, best_alpha\n",
        "  elif type == 'lasso':\n",
        "    best_alpha = alpha_search(X, Y, alpha = alpha, type = 'lasso')\n",
        "    model_lasso = model.fit_regularized(L1_wt=1, alpha= best_alpha, start_params=results_fu.params)\n",
        "    lasso_result = sm.regression.linear_model.OLSResults(model, model_lasso.params, model.normalized_cov_params)\n",
        "    return lasso_result, best_alpha\n",
        "  else:\n",
        "    return results_fu, Best_alpha\n",
        "\n",
        "\n",
        "@ignore_warnings(category=[ConvergenceWarning, UserWarning])\n",
        "def backward_selection(df, y_label = 'Close', previous = False, shift_n = 1, preprocess = 'standardize', intercept = True, type = 'lasso', alpha = [0.01,0.05, 0.1, 0.5, 1], threshold = 0.05):\n",
        "  X, Y, scaler = get_data(df, y_label = y_label, previous = previous, shift_n = shift_n, preprocess = preprocess, intercept = intercept)\n",
        "\n",
        "  #create linear model\n",
        "  model, best_alpha = liner_model(X, Y, type = 'simple', alpha = None)\n",
        "  \n",
        "  #backward selection model\n",
        "  # .1 get feature coef result\n",
        "  res = list(model.pvalues)\n",
        "  max_p = max(res)\n",
        "\n",
        "  # .2 find the biggest coef and correlated feature name\n",
        "  while max_p > threshold:\n",
        "    ind = res.index(max_p) # the index of max p value\n",
        "    col = X.columns[ind] # find the column name\n",
        "\n",
        "  # .3 remove the feature from X\n",
        "    X = X.drop(col, axis = 1)\n",
        "  # .4 build a new model\n",
        "    if len(X.columns) == 0:\n",
        "      print('all features have been removed, return the last avaiable model')\n",
        "      return model, X, best_alpha, scaler\n",
        "    model, best_alpha = liner_model(X, Y, type = 'simple', alpha = None)\n",
        "    res = list(model.pvalues)\n",
        "    max_p = max(res)\n",
        "\n",
        "  #return result\n",
        "  return model, X, best_alpha, scaler"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 4: PCA model\n",
        "def pca(x, feature_n = None):\n",
        "  pca = PCA(n_components = feature_n)\n",
        "  res = pca.fit_transform(x)\n",
        "  return res\n",
        "\n",
        "def pca_model(df, y_label, threshold = 0.05, feature_n = None):\n",
        "  X, Y, scaler = get_data(df, y_label, preprocess = 'standardize', intercept = None)\n",
        "  x = pca(X, feature_n)\n",
        "  df_ = pd.concat([pd.DataFrame(x), pd.DataFrame({f'{y_label}':Y})], axis=1)\n",
        "  x, y, model_result, remove_list, best_alpha = backward_selection(df_, y_label =  y_label, type = 'simple', alpha = None, threshold = threshold, preprocess = None, intercept = True)\n",
        "  print(model_result.summary)\n",
        "  return  x, y, model_result, scaler"
      ],
      "metadata": {
        "id": "x9Oi5Z5qt8Bv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 5: dataset and plot classes\n",
        "class agg_df:\n",
        "  def __init__(self, token_path, transaction_path, index_path, commodity_path, fixed_features, filter_features, token_name):\n",
        "    self.token_path =  token_path\n",
        "    self.transaction_path = transaction_path\n",
        "    self.index_path = index_path\n",
        "    self.commodity_path = commodity_path\n",
        "    self.fixed_features = fixed_features\n",
        "    self.filter_features = filter_features\n",
        "    self.token_name = token_name\n",
        "\n",
        "  def get_df(self):\n",
        "    target_df, missing_df  = generate_target_df(self.token_path,\n",
        "                                                self.transaction_path,\n",
        "                                                self.index_path,\n",
        "                                                self.commodity_path, \n",
        "                                                self.fixed_features, \n",
        "                                                self.filter_features, \n",
        "                                                self.token_name)\n",
        "    return target_df\n",
        "\n",
        "  def get_na_check(self):\n",
        "    target_df, missing_df  = generate_target_df(self.token_path,\n",
        "                                                self.transaction_path,\n",
        "                                                self.index_path,\n",
        "                                                self.commodity_path, \n",
        "                                                self.fixed_features, \n",
        "                                                self.filter_features, \n",
        "                                                self.token_name)\n",
        "    print('NA check result:')\n",
        "    print(missing_df[missing_df['Total'] >= 1])\n",
        "    return missing_df\n",
        "\n",
        "\n",
        "class plots:\n",
        "  def __init__(self, df):\n",
        "    self.df = df.copy()\n",
        "\n",
        "  def get_lineplot(self, cols, y, second_axis = True):\n",
        "    draw_lineplot(self.df , cols, y, second_axis = second_axis)\n",
        "\n",
        "  def get_cor_matrix(self):\n",
        "    correlation_matrix(self.df)"
      ],
      "metadata": {
        "id": "anOsjwBGJDyT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module 6: model class\n",
        "class backward_selection_model:\n",
        "  def __init__(self, df, y_label = 'Close', previous = False, shift_n = 1, preprocess = 'standardize', intercept = True, type = 'lasso', alpha = [0.01,0.05, 0.1, 0.5, 1], threshold = 0.05):\n",
        "    self.df = df.copy()\n",
        "    self.y_label = y_label\n",
        "    self.previous = previous\n",
        "    self.shift_n = shift_n\n",
        "    self.preprocess = preprocess\n",
        "    self.intercept = intercept\n",
        "    self.type = type\n",
        "    self.alpha = alpha\n",
        "    self.threshold = threshold\n",
        "    model, X, best_alpha, scaler = backward_selection(self.df, y_label = self.y_label, previous = self.previous, \n",
        "                                                      shift_n = self.shift_n, preprocess = self.preprocess, intercept = self.intercept, \n",
        "                                                      type = self.type, alpha = self.alpha, threshold = self.threshold)\n",
        "    self.model = model\n",
        "    self.X = X\n",
        "    self.best_alpha = best_alpha,\n",
        "    self.scaler = scaler\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.model\n",
        "  \n",
        "  def get_final_features(self):\n",
        "    return self.X.columns\n",
        "  \n",
        "  def get_best_alpha(self):\n",
        "    return self.best_alpha\n",
        "\n",
        "  def get_scaler(self):\n",
        "    return self.scaler\n",
        "\n",
        "  def get_prediction(self, target_df):\n",
        "    scaler = self.get_scaler()\n",
        "    target_cols = list(self.get_final_features())\n",
        "    model = self.get_model()\n",
        "\n",
        "    if self.previous == True:\n",
        "      target_df = df_shift(target_df, y_label = self.y_label, shift_n = self.shift_n)\n",
        "  \n",
        "    target_X = target_df.loc[:, target_df.columns != self.y_label]\n",
        "    target_Y = target_df.loc[:, target_df.columns == self.y_label]\n",
        "  \n",
        "    if self.preprocess == 'standardize' or self.preprocess == 'minmax':\n",
        "      X_ = scaler.transform(target_X)\n",
        "      target_X = pd.DataFrame(X_, columns= target_X.columns, index =  target_X.index)\n",
        "\n",
        "    if self.intercept == True:\n",
        "      target_X = sm.add_constant(target_X)\n",
        "    \n",
        "    target_X = target_X.loc[:, target_cols]\n",
        "\n",
        "    return model.predict(target_X)\n",
        "\n",
        "  def get_mse(self, target_df):\n",
        "    prediction = self.get_prediction(target_df)\n",
        "\n",
        "    if self.previous == True:\n",
        "      target_df = df_shift(target_df, y_label = self.y_label, shift_n = self.shift_n)\n",
        "\n",
        "    target_y = target_df.loc[:, target_df.columns == self.y_label].values.ravel()\n",
        "\n",
        "    return mse(target_y, prediction)\n",
        "\n",
        "  def get_coef_df(self):\n",
        "    return pd.DataFrame({'coef': self.model.params, 'P-value': self.model.pvalues})\n"
      ],
      "metadata": {
        "id": "dZbYaO8-EBcm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_path = 'https://raw.githubusercontent.com/Carloszone/Crypto_Token_Research/main/datasets/token_df.csv'\n",
        "transaction_path = 'https://raw.githubusercontent.com/Carloszone/Crypto_Token_Research/main/datasets/transaction_df.csv'\n",
        "index_path = 'https://raw.githubusercontent.com/Carloszone/Crypto_Token_Research/main/datasets/index_df.csv'\n",
        "commodity_path = 'https://raw.githubusercontent.com/Carloszone/Crypto_Token_Research/main/datasets/commodity_df.csv'\n",
        "\n",
        "fixed_features = ['Volume', 'weekend']\n",
        "filter_features = re.compile('^Close.*')\n",
        "\n",
        "tokens = ['Ethereum', 'Chainlink', 'Wrapped Bitcoin', '0x', 'Maker', 'Augur', 'Golem', 'Loopring', 'Basic Attention Token', 'Bitcoin', 'Binance Coin']"
      ],
      "metadata": {
        "id": "Dd0bcjX5baIY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg = pd.DataFrame({'Token': [], 'Model_Type': [], 'RMSE': [], 'R-Square Adj': []})\n",
        "\n",
        "token = 'Ethereum'\n",
        "\n",
        "token_info = agg_df(token_path, transaction_path, index_path, commodity_path, fixed_features, filter_features, token)\n",
        "token_df = token_info.get_df()\n",
        "# split df\n",
        "train_df, test_df = split_df(token_df, timedate = '2021-11-06')\n",
        "\n",
        "# build static model\n",
        "print('Static Model')\n",
        "s_model = backward_selection_model(train_df)\n",
        "agg.loc[len(agg.index)] = [token, 'Static', np.sqrt(s_model.get_mse(test_df)), s_model.get_model().rsquared_adj]\n",
        "filename = token + ' Static.csv'\n",
        "res = s_model.get_coef_df()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOBMx3Q4ka8z",
        "outputId": "a5478c1e-c623-44bf-f4a5-7e84b19c31bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agg = pd.DataFrame({'Token': [], 'Model_Type': [],'Volume': [], 'Transaction Fee coef': [], 'Priority Fee coef': [], 'RMSE': [], 'R-Square Adj': []})\n",
        "\n",
        "for token in tokens:\n",
        "  print('\\nToken name:', token)\n",
        "  print('Process: ', tokens.index(token)+1,'/', len(tokens))\n",
        "\n",
        "  # get token dataframe\n",
        "  token_info = agg_df(token_path, transaction_path, index_path, commodity_path, fixed_features, filter_features, token)\n",
        "  token_df = token_info.get_df()\n",
        "  # split df\n",
        "  train_df, test_df = split_df(token_df, timedate = '2021-11-06')\n",
        "\n",
        "  # build static model\n",
        "  s_model = backward_selection_model(train_df)\n",
        "  res = s_model.get_coef_df()\n",
        "  if 'Volume' in res.index:\n",
        "    v_valume = res.loc['Volume',:][0]\n",
        "  else:\n",
        "    v_valume = 'NA'\n",
        "\n",
        "  if 'avg_transaction_fee_per_transaction' in res.index:\n",
        "    v_transaction = res.loc['avg_transaction_fee_per_transaction',:][0]\n",
        "  else:\n",
        "    v_transaction = 'NA'\n",
        "\n",
        "  if  'avg_priority_fee_per_transaction' in res.index:\n",
        "    v_priority = res.loc['avg_priority_fee_per_transaction',:][0]\n",
        "  else:\n",
        "    v_priority = 'NA'\n",
        "  agg.loc[len(agg.index)] = [token, 'Static', v_valume, v_transaction, v_priority, np.sqrt(s_model.get_mse(test_df)), s_model.get_model().rsquared_adj]\n",
        "\n",
        "  # build dynamic model\n",
        "  d_model = backward_selection_model(train_df, previous = True)\n",
        "  res = d_model.get_coef_df()\n",
        "  if 'Volume' in res.index:\n",
        "    v_valume = res.loc['Volume',:][0]\n",
        "  else:\n",
        "    v_valume = 'NA'\n",
        "\n",
        "  if 'avg_transaction_fee_per_transaction' in res.index:\n",
        "    v_transaction = res.loc['avg_transaction_fee_per_transaction',:][0]\n",
        "  else:\n",
        "    v_transaction = 'NA'\n",
        "\n",
        "  if  'avg_priority_fee_per_transaction' in res.index:\n",
        "    v_priority = res.loc['avg_priority_fee_per_transaction',:][0]\n",
        "  else:\n",
        "    v_priority = 'NA'\n",
        "  agg.loc[len(agg.index)] = [token, 'Dynamic', v_valume, v_transaction, v_priority, np.sqrt(d_model.get_mse(test_df)), d_model.get_model().rsquared_adj]\n",
        "agg.to_csv('result.csv')\n",
        "files.download('result.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ifpc8S9ibqVu",
        "outputId": "0c6a12ee-80bd-4301-abce-a55d68aebe7a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Ethereum\n",
            "Process:  1 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Chainlink\n",
            "Process:  2 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Wrapped Bitcoin\n",
            "Process:  3 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: 0x\n",
            "Process:  4 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Maker\n",
            "Process:  5 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Augur\n",
            "Process:  6 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Golem\n",
            "Process:  7 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Loopring\n",
            "Process:  8 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Basic Attention Token\n",
            "Process:  9 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Bitcoin\n",
            "Process:  10 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token name: Binance Coin\n",
            "Process:  11 / 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
            "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_7eb6074e-9464-417a-bb40-e993a9f8adce\", \"result.csv\", 2099)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}